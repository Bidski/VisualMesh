# Section that defines the dataset files.
dataset:
  # Training file, used for training the network.
  training:
    paths:
      - dataset/spotlight_training.tfrecord

    # A map which converts internal feature keys to the external dataset keys
    keys:
      Hoc: camera/Hoc
      lens/centre: camera/centre
      lens/focal_length: camera/focal_length
      lens/fov: camera/fov
      lens/k: camera/k
      lens/projection: camera/projection

    # Additional configuration that will be merged into the other configurations for this dataset
    config:
      orientation:
        variants:
          # Adjustments to the height of the mesh above the observation plane.
          height: { mean: 0, stddev: 0.05 }
          # Adjustments to the orientation of the observation plane
          # We choose a random axis and rotate by a random angle from this distribution
          rotation: { mean: 0, stddev: 0.0872665 }

  # Validation file, used to inspect progress of the network as it is training.
  # It is also used as the source for progress images.
  validation:
    paths:
      - dataset/spotlight_validation.tfrecord

  # Test dataset, used for performing final evaluations on the performance of the network.
  testing:
    paths:
      - dataset/spotlight_testing.tfrecord

# What camera configuration we have (Monoscopic, Stereoscopic)
view:
  type: Monoscopic
  config: {}

# What kind of examples we are using for input data (Image)
example:
  type: Image
  config: {}

# What kind of mesh orientation we are using (Ground, Spotlight)
orientation:
  type: Ground
  config: {}

# The type of output we are training the network for (Classification, Seeker)
label:
  type: Classification
  config:
    # The classes we are using for classification
    classes:
      - name: ball
        colours:
          - [255, 0, 0]
      - name: goal
        colours:
          - [255, 255, 0]
      - name: line
        colours:
          - [255, 255, 255]
      - name: field
        colours:
          - [0, 255, 0]
      - name: environment
        colours:
          - [0, 0, 0]

# What kind of graph we are projecting (VisualMesh)
projection:
  type: VisualMesh
  # The configuration for the various components of the flavour
  config:
    # The type of mesh we are using
    mesh:
      # The type of Visual Mesh we are generating
      model: RING6
      # How many distinct meshes to cache before dropping old ones
      cached_meshes: 100
      # The maximum distance the Visual Mesh will be projected for. This should be slightly further than the most distant
      # object that you wish to detect to account for noises in the projection.
      max_distance: 20
      # The geometry of the object we are detecting in the Visual Mesh.
    geometry:
      # The shape to project, either CIRCLE or SPHERE.
      shape: SPHERE
      # The radius of the object to be detected.
      radius: 0.0949996
      # How many intersections with the target object we should have.
      intersections: 6
      # How many intersections the mesh can vary by before it will generate a new mesh
      intersection_tolerance: 0.5

# Configuration for the convolutional neural network.
network:
  # The network structure defined as a graph of ops where each element will define its inputs.
  # There are two predefined inputs to the graph which are `X` and `G` which represent the input values to the network
  # and the neighbourhood graph respectively.
  # There should exist a node named `output` that defines the final output of the network.
  # The network will start at this element and work back from there to find which components are needed and in what order.
  # For each of the elements they will define an operation they are doing, and optionally provide options for constructing
  # The final output dimensionality (which will be the size of the dataset output) can be accessed with `$output_dims`
  # This variable name if placed anywhere in the structure options will be replaced with the integer number of outputs
  # the dataset will produce
  structure:
    g1: { op: GraphConvolution, inputs: [X, G] }
    d1: { op: Dense, inputs: [g1], options: { units: 16, activation: selu, kernel_initializer: lecun_normal } }
    g2: { op: GraphConvolution, inputs: [d1, G] }
    d2: { op: Dense, inputs: [g2], options: { units: 16, activation: selu, kernel_initializer: lecun_normal } }
    g3: { op: GraphConvolution, inputs: [d2, G] }
    d3: { op: Dense, inputs: [g3], options: { units: 16, activation: selu, kernel_initializer: lecun_normal } }
    g4: { op: GraphConvolution, inputs: [d3, G] }
    d4: { op: Dense, inputs: [g4], options: { units: 16, activation: selu, kernel_initializer: lecun_normal } }
    g5: { op: GraphConvolution, inputs: [d4, G] }
    d5: { op: Dense, inputs: [g5], options: { units: 16, activation: selu, kernel_initializer: lecun_normal } }
    g6: { op: GraphConvolution, inputs: [d5, G] }
    d6: { op: Dense, inputs: [g6], options: { units: 16, activation: selu, kernel_initializer: lecun_normal } }
    g7: { op: GraphConvolution, inputs: [d6, G] }
    d7: { op: Dense, inputs: [g7], options: { units: 16, activation: selu, kernel_initializer: lecun_normal } }
    g8: { op: GraphConvolution, inputs: [d7, G] }
    d8: { op: Dense, inputs: [g8], options: { units: 8, activation: selu, kernel_initializer: lecun_normal } }
    g9: { op: GraphConvolution, inputs: [d8, G] }
    d9: { op: Dense, inputs: [g9], options: { units: 8, activation: selu, kernel_initializer: lecun_normal } }
    g10: { op: GraphConvolution, inputs: [d9, G] }
    d10: { op: Dense, inputs: [g10], options: { units: 8, activation: selu, kernel_initializer: lecun_normal } }
    g11: { op: GraphConvolution, inputs: [d10, G] }
    d11: { op: Dense, inputs: [g11], options: { units: 8, activation: selu, kernel_initializer: lecun_normal } }
    g12: { op: GraphConvolution, inputs: [d11, G] }
    d12: { op: Dense, inputs: [g12], options: { units: 8, activation: selu, kernel_initializer: lecun_normal } }
    g13: { op: GraphConvolution, inputs: [d12, G] }
    output: { op: Dense, inputs: [g13], options: { units: $output_dims, activation: softmax } }

# Testing
testing:
  # How many raw elements to process at a time during testing
  batch_size: 200
  # The number of bins to use when calculating curves
  n_bins: 1000

# Settings to use when training the network
training:
  # The batch size to use when training the network (how many images to feed in a single training step).
  # Do not make this number too large or it will create poor gradients.
  batch_size: 20
  # Number of batches to consider an epoch, if None then it is the length of the input dataset
  batches_per_epoch: 1000
  # Number of epochs to execute
  epochs: 500
  # Optimiser settings
  optimiser:
    type: Ranger
    learning_rate: 1e-3
    sync_period: 6
    slow_step_size: 0.5

  # optimiser:
  #   type: Adam
  #   learning_rate: 1e-3

  # Settings for the validation step of the network.
  validation:
    # How big the batch should be for validation step.
    batch_size: 20
    # How many batches to load for the validation step
    samples: 10
    # How many images to show in tensorboard, they are taken as the first n images of the validation set.
    progress_images: 20
