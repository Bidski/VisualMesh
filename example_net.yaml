# Section that defines the dataset files.
dataset:
  # Training file, used for training the network.
  training: dataset/training.tfrecord
  # Validation file, used to inspect progress of the network as it is training.
  # It is also used as the source for progress images.
  validation: dataset/validation.tfrecord
  # Test dataset, used for performing final evaluations on the performance of the network.
  testing: dataset/testing.tfrecord

# The model and geometry we are using
model:
  # The type of model that we are using to generate the Visual Mesh
  mesh:
    # The type of Visual Mesh we are generating
    type: RING6
    # How many distinct meshes to cache before dropping old ones
    cached_meshes: 100
    # The maximum distance the Visual Mesh will be projected for. This should be slightly further than the most distant
    # object that you wish to detect to account for noises in the projection.
    max_distance: 20

  # The geometry of the object we are detecting in the Visual Mesh.
  geometry:
    # The shape to project, either CIRCLE or SPHERE.
    shape: SPHERE
    # The radius of the object to be detected.
    radius: 0.0949996
    # How many intersections with the target object we should have.
    intersections: 6
    # How many intersections the mesh can vary by before it will generate a new mesh
    intersection_tolerance: 0.5

# Configuration for the convolutional neural network.
network:
  # The network structure defined as a graph of ops where each element will define its inputs.
  # There are two predefined inputs to the graph which are `X` and `G` which represent the input values to the network
  # and the neighbourhood graph respecitvely.
  # There should exist a node named `output` that defines the final output of the network.
  # The network will start at this element and work back from there to find which components are needed and in what order.
  # For each of the elements they will define an operation they are doing, and optionally provide options for constructing
  structure:
    g1: { op: GraphConvolution, inputs: ["X", "G"] }
    d1: { op: Dense, inputs: [g1], options: { units: 16, activation: selu, kernel_initializer: lecun_normal } }
    g2: { op: GraphConvolution, inputs: ["d1", "G"] }
    d2: { op: Dense, inputs: [g2], options: { units: 16, activation: selu, kernel_initializer: lecun_normal } }
    g3: { op: GraphConvolution, inputs: ["d2", "G"] }
    d3: { op: Dense, inputs: [g3], options: { units: 16, activation: selu, kernel_initializer: lecun_normal } }
    g4: { op: GraphConvolution, inputs: ["d3", "G"] }
    d4: { op: Dense, inputs: [g4], options: { units: 16, activation: selu, kernel_initializer: lecun_normal } }
    g5: { op: GraphConvolution, inputs: ["d4", "G"] }
    d5: { op: Dense, inputs: [g5], options: { units: 16, activation: selu, kernel_initializer: lecun_normal } }
    g6: { op: GraphConvolution, inputs: ["d5", "G"] }
    d6: { op: Dense, inputs: [g6], options: { units: 16, activation: selu, kernel_initializer: lecun_normal } }
    g7: { op: GraphConvolution, inputs: ["d6", "G"] }
    d7: { op: Dense, inputs: [g7], options: { units: 16, activation: selu, kernel_initializer: lecun_normal } }
    g8: { op: GraphConvolution, inputs: ["d7", "G"] }
    d8: { op: Dense, inputs: [g8], options: { units: 8, activation: selu, kernel_initializer: lecun_normal } }
    g9: { op: GraphConvolution, inputs: ["d8", "G"] }
    d9: { op: Dense, inputs: [g9], options: { units: 8, activation: selu, kernel_initializer: lecun_normal } }
    g10: { op: GraphConvolution, inputs: ["d9", "G"] }
    d10: { op: Dense, inputs: [g10], options: { units: 8, activation: selu, kernel_initializer: lecun_normal } }
    g11: { op: GraphConvolution, inputs: ["d10", "G"] }
    d11: { op: Dense, inputs: [g11], options: { units: 8, activation: selu, kernel_initializer: lecun_normal } }
    g12: { op: GraphConvolution, inputs: ["d11", "G"] }
    d12: { op: Dense, inputs: [g12], options: { units: 8, activation: selu, kernel_initializer: lecun_normal } }
    g13: { op: GraphConvolution, inputs: ["d12", "G"] }
    output: { op: Dense, inputs: [g13], options: { units: n_classes, activation: softmax } }

  # This is a mapping of the classes to the colours that represent them in the mask images.
  # Each of the elements has a name, and one or more RGB values that represent it in the mask image.
  # The size of the final output layer will be the same as the order provided in this list.
  classes:
    - name: ball
      colours:
        - [255, 0, 0]
    - name: goal
      colours:
        - [255, 255, 0]
    - name: line
      colours:
        - [255, 255, 255]
    - name: field
      colours:
        - [0, 255, 0]
        - [255, 255, 255] # Field line is still field
    - name: environment
      colours:
        - [0, 0, 0]

# Testing
testing:
  # How many raw elements to process at a time during testing
  batch_size: 200
  # The number of bins to use when calculating curves
  n_bins: 1000

# Settings to use when training the network
training:
  # The batch size to use when training the network (how many images to feed in a single training step).
  # Do not make this number too large or it will create poor gradients.
  batch_size: 20
  # Number of batches to consider an epoch, if None then it is the length of the input dataset
  batches_per_epoch: 1000
  # Number of epochs to execute
  epochs: 500
  # Optimiser settings
  optimiser:
    type: Ranger
    learning_rate: 1e-3
    sync_period: 6
    slow_step_size: 0.5

  # optimiser:
  #   type: Adam
  #   learning_rate: 1e-3

  # Settings for the validation step of the network.
  validation:
    # How big the batch should be for validation step.
    batch_size: 20
    # How many batches to load for the validation step
    samples: 10
    # How many images to show in tensorboard, they are taken as the first n images of the validation set.
    progress_images: 20

  # Variations to apply to the training data, note this only applies to training data and not validation.
  # They are defined using a mean value and a standard deviation using a truncated normal distribution.
  # If the values are greater than 2 standard deviations from the mean they will be resampled.
  variants:
    mesh:
      # Adjustments to the height of the mesh above the observation plane.
      height: { "mean": 0, "stddev": 0.05 }
      # Adjustments to the orientation of the observation plane.
      # This value is used for three euler rotations added to the camera rotation in radians.
      rotation: { "mean": 0, "stddev": 0.0872665 }
